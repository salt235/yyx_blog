<!doctype html><html lang="zh-CN"><head><meta charset="utf-8" /><meta name="viewport" content="width=device-width,initial-scale=1" /><meta name="generator" content="VuePress 2.0.0-rc.26" /><meta name="theme" content="VuePress Theme Plume 1.0.0-rc.183" /><script id="check-mac-os">document.documentElement.classList.toggle('mac', /Mac|iPhone|iPod|iPad/i.test(navigator.platform))</script><script id="check-dark-mode">;(function () {const um= localStorage.getItem('vuepress-theme-appearance') || 'dark';const sm = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;const isDark = um === 'dark' || (um !== 'light' && sm);document.documentElement.dataset.theme = isDark ? 'dark' : 'light';})();</script><link rel="icon" type="image/png" href="/yuzu.png"><link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin=""><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lxgw-wenkai-webfont@1.7.0/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lxgw-wenkai-mono-webfont@1.7.0/style.css"><title>transformer | yyx_blog</title><meta name="description" content="my blog"><link rel="preload" href="/assets/style-CBr75pm-.css" as="style"><link rel="stylesheet" href="/assets/style-CBr75pm-.css"><link rel="modulepreload" href="/assets/app-DCYTdW2o.js"><link rel="modulepreload" href="/assets/index.html-CtWW-Qrr.js"></head><body><div id="app"><!--[--><!--[--><div class="theme-plume vp-layout" vp-container data-v-06138826><!--[--><!--[--><!--]--><!--[--><span tabindex="-1" data-v-9e2340c1></span><a href="#VPContent" class="vp-skip-link visually-hidden" data-v-9e2340c1> Skip to content </a><!--]--><!----><header class="vp-nav" data-v-06138826 data-v-536e2c68><div class="vp-navbar" vp-navbar data-v-536e2c68 data-v-c7a2c3b6><div class="wrapper" data-v-c7a2c3b6><div class="container" data-v-c7a2c3b6><div class="title" data-v-c7a2c3b6><div class="vp-navbar-title has-sidebar" data-v-c7a2c3b6 data-v-980090b1><a class="vp-link link no-icon title" href="/" data-v-980090b1><!--[--><!--[--><!--]--><!--[--><!--[--><!--[--><img class="vp-image dark logo" style="" src="/plume1.png" alt data-v-49f73b50><!--]--><!--[--><img class="vp-image light logo" style="" src="/plume1.png" alt data-v-49f73b50><!--]--><!--]--><!--]--><span data-v-980090b1>yyx_blog</span><!--[--><!--]--><!--]--></a></div></div><div class="content" data-v-c7a2c3b6><div class="content-body" data-v-c7a2c3b6><!--[--><!--]--><div class="vp-navbar-search search" data-v-c7a2c3b6><div class="search-wrapper" data-v-d1b35e6a><!----><div id="local-search" data-v-d1b35e6a><button type="button" class="mini-search mini-search-button" aria-label="搜索文档" data-v-d1b35e6a><span class="mini-search-button-container"><span class="mini-search-search-icon vpi-mini-search" aria-label="search icon"></span><span class="mini-search-button-placeholder">搜索文档</span></span><span class="mini-search-button-keys"><kbd class="mini-search-button-key"></kbd><kbd class="mini-search-button-key">K</kbd></span></button></div></div></div><!--[--><!--]--><nav aria-labelledby="main-nav-aria-label" class="vp-navbar-menu menu" data-v-c7a2c3b6 data-v-bea7f34f><span id="main-nav-aria-label" class="visually-hidden" data-v-bea7f34f>Main Navigation</span><!--[--><!--[--><a class="vp-link link navbar-menu-link" href="/" tabindex="0" data-v-bea7f34f data-v-8f70c765><!--[--><!----><span data-v-8f70c765>首页</span><!----><!--]--></a><!--]--><!--[--><a class="vp-link link navbar-menu-link" href="/blog/" tabindex="0" data-v-bea7f34f data-v-8f70c765><!--[--><!----><span data-v-8f70c765>博客</span><!----><!--]--></a><!--]--><!--[--><div class="vp-flyout vp-navbar-menu-group" data-v-bea7f34f data-v-21b2957d><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-21b2957d><span class="text" data-v-21b2957d><!----><!----><span data-v-21b2957d>笔记</span><!----><span class="vpi-chevron-down text-icon" data-v-21b2957d></span></span></button><div class="menu" data-v-21b2957d><div class="vp-menu" data-v-21b2957d data-v-2ada254e><div class="items" data-v-2ada254e><!--[--><!--[--><div class="vp-menu-link" data-v-2ada254e data-v-e17ba65a><a class="vp-link link" href="/notes/Java/" data-v-e17ba65a><!--[--><!----> Java <!----><!--]--></a></div><!--]--><!--[--><div class="vp-menu-link" data-v-2ada254e data-v-e17ba65a><a class="vp-link link" href="/notes/Lee_DL/" data-v-e17ba65a><!--[--><!----> DL <!----><!--]--></a></div><!--]--><!--[--><div class="vp-menu-link" data-v-2ada254e data-v-e17ba65a><a class="vp-link link" href="/notes/ROS/" data-v-e17ba65a><!--[--><!----> ROS <!----><!--]--></a></div><!--]--><!--[--><div class="vp-menu-link" data-v-2ada254e data-v-e17ba65a><a class="vp-link link" href="/notes/VLN/" data-v-e17ba65a><!--[--><!----> VLN <!----><!--]--></a></div><!--]--><!--[--><div class="vp-menu-link" data-v-2ada254e data-v-e17ba65a><a class="vp-link link" href="/notes/Papers/" data-v-e17ba65a><!--[--><!----> 文献阅读 <!----><!--]--></a></div><!--]--><!--[--><div class="vp-menu-link" data-v-2ada254e data-v-e17ba65a><a class="vp-link link" href="/notes/Misc/" data-v-e17ba65a><!--[--><!----> 杂项笔记&amp;记录 <!----><!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><a class="vp-link link vp-external-link-icon navbar-menu-link" href="https://run.yyx235.top/" target="_blank" rel="noreferrer" tabindex="0" data-v-bea7f34f data-v-8f70c765><!--[--><!----><span data-v-8f70c765>跑步页面</span><!----><!--]--></a><!--]--><!--[--><a class="vp-link link navbar-menu-link" href="/youji/" tabindex="0" data-v-bea7f34f data-v-8f70c765><!--[--><!----><span data-v-8f70c765>柚记</span><!----><!--]--></a><!--]--><!--[--><a class="vp-link link navbar-menu-link" href="/friends/" tabindex="0" data-v-bea7f34f data-v-8f70c765><!--[--><!----><span data-v-8f70c765>友链</span><!----><!--]--></a><!--]--><!--]--></nav><!--[--><!--]--><!----><div class="vp-navbar-appearance appearance" data-v-c7a2c3b6 data-v-55c747f2><button class="vp-switch vp-switch-appearance" type="button" role="switch" title aria-checked="false" data-v-55c747f2 data-v-8ac8d10b data-v-ce6aaf63><span class="check" data-v-ce6aaf63><span class="icon" data-v-ce6aaf63><!--[--><span class="vpi-sun sun" data-v-8ac8d10b></span><span class="vpi-moon moon" data-v-8ac8d10b></span><!--]--></span></span></button></div><div class="vp-social-links vp-navbar-social-links social-links" data-v-c7a2c3b6 data-v-6d5034e3 data-v-e3a04c34><!--[--><a class="vp-social-link no-icon" href="https://github.com/salt235" aria-label="github" title="github" target="_blank" rel="noopener" data-v-e3a04c34 data-v-6f4b389b><!----></a><!--]--></div><div class="vp-flyout vp-navbar-extra extra" data-v-c7a2c3b6 data-v-e32dcd80 data-v-21b2957d><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-21b2957d><span class="vpi-more-horizontal icon" data-v-21b2957d></span></button><div class="menu" data-v-21b2957d><div class="vp-menu" data-v-21b2957d data-v-2ada254e><!----><!--[--><!--[--><!----><div class="group" data-v-e32dcd80><div class="item appearance" data-v-e32dcd80><p class="label" data-v-e32dcd80>外观</p><div class="appearance-action" data-v-e32dcd80><button class="vp-switch vp-switch-appearance" type="button" role="switch" title aria-checked="false" data-v-e32dcd80 data-v-8ac8d10b data-v-ce6aaf63><span class="check" data-v-ce6aaf63><span class="icon" data-v-ce6aaf63><!--[--><span class="vpi-sun sun" data-v-8ac8d10b></span><span class="vpi-moon moon" data-v-8ac8d10b></span><!--]--></span></span></button></div></div></div><div class="group" data-v-e32dcd80><div class="item social-links" data-v-e32dcd80><div class="vp-social-links social-links-list" data-v-e32dcd80 data-v-e3a04c34><!--[--><a class="vp-social-link no-icon" href="https://github.com/salt235" aria-label="github" title="github" target="_blank" rel="noopener" data-v-e3a04c34 data-v-6f4b389b><!----></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="vp-navbar-hamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="nav-screen" data-v-c7a2c3b6 data-v-21c78606><span class="container" data-v-21c78606><span class="top" data-v-21c78606></span><span class="middle" data-v-21c78606></span><span class="bottom" data-v-21c78606></span></span></button></div></div></div></div><div class="divider" data-v-c7a2c3b6><div class="divider-line" data-v-c7a2c3b6></div></div></div><!----></header><div class="vp-local-nav reached-top" data-v-06138826 data-v-7769f651><button class="menu" aria-expanded="false" aria-controls="SidebarNav" data-v-7769f651><span class="vpi-align-left menu-icon" data-v-7769f651></span><span class="menu-text" data-v-7769f651>Menu</span></button><div class="vp-local-nav-outline-dropdown" style="--vp-vh:0px;" data-v-7769f651 data-v-df613a3b><!----><!----></div></div><aside class="vp-sidebar" vp-sidebar data-v-06138826 data-v-78bbb0f4><div class="curtain" data-v-78bbb0f4></div><nav id="SidebarNav" class="nav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-78bbb0f4><span id="sidebar-aria-label" class="visually-hidden" data-v-78bbb0f4> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="no-transition group" data-v-1935dc3f><section class="vp-sidebar-item sidebar-item level-0" data-v-1935dc3f data-v-e475df9d><!----><div data-v-e475df9d data-v-e475df9d><div class="items" data-v-e475df9d><!--[--><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-e475df9d data-v-e475df9d><div class="item" data-v-e475df9d><div class="indicator" data-v-e475df9d></div><!----><a class="vp-link link link" href="/notes/Lee_DL/" data-v-e475df9d><!--[--><p class="text" data-v-e475df9d><span data-v-e475df9d>前言</span><!----></p><!--]--></a><!----></div><!----></div><!--]--></div></div></section></div><div class="no-transition group" data-v-1935dc3f><section class="vp-sidebar-item sidebar-item level-0 collapsible has-active" data-v-1935dc3f data-v-e475df9d><div class="item" role="button" tabindex="0" data-v-e475df9d><div class="indicator" data-v-e475df9d></div><!----><h2 class="text" data-v-e475df9d><span data-v-e475df9d>Attention-based Models</span><!----></h2><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-e475df9d><span class="vpi-chevron-right caret-icon" data-v-e475df9d></span></div></div><div data-v-e475df9d data-v-e475df9d><div class="items" data-v-e475df9d><!--[--><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-e475df9d data-v-e475df9d><div class="item" data-v-e475df9d><div class="indicator" data-v-e475df9d></div><!----><a class="vp-link link link" href="/notes/Lee_DL/61oxdhhi/" data-v-e475df9d><!--[--><p class="text" data-v-e475df9d><span data-v-e475df9d>Bert</span><!----></p><!--]--></a><!----></div><!----></div><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-e475df9d data-v-e475df9d><div class="item" data-v-e475df9d><div class="indicator" data-v-e475df9d></div><!----><a class="vp-link link link" href="/notes/Lee_DL/tun4uuvu/" data-v-e475df9d><!--[--><p class="text" data-v-e475df9d><span data-v-e475df9d>GPT</span><!----></p><!--]--></a><!----></div><!----></div><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-e475df9d data-v-e475df9d><div class="item" data-v-e475df9d><div class="indicator" data-v-e475df9d></div><!----><a class="vp-link link link" href="/notes/Lee_DL/1unm9yzt/" data-v-e475df9d><!--[--><p class="text" data-v-e475df9d><span data-v-e475df9d>Self-attention</span><!----></p><!--]--></a><!----></div><!----></div><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-e475df9d data-v-e475df9d><div class="item" data-v-e475df9d><div class="indicator" data-v-e475df9d></div><!----><a class="vp-link link link" href="/notes/Lee_DL/w48llyos/" data-v-e475df9d><!--[--><p class="text" data-v-e475df9d><span data-v-e475df9d>transformer</span><!----></p><!--]--></a><!----></div><!----></div><!--]--></div></div></section></div><div class="no-transition group" data-v-1935dc3f><section class="vp-sidebar-item sidebar-item level-0 collapsible" data-v-1935dc3f data-v-e475df9d><div class="item" role="button" tabindex="0" data-v-e475df9d><div class="indicator" data-v-e475df9d></div><!----><h2 class="text" data-v-e475df9d><span data-v-e475df9d>CNN</span><!----></h2><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-e475df9d><span class="vpi-chevron-right caret-icon" data-v-e475df9d></span></div></div><div data-v-e475df9d data-v-e475df9d><div class="items" data-v-e475df9d><!--[--><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-e475df9d data-v-e475df9d><div class="item" data-v-e475df9d><div class="indicator" data-v-e475df9d></div><!----><a class="vp-link link link" href="/notes/Lee_DL/528u1pz0/" data-v-e475df9d><!--[--><p class="text" data-v-e475df9d><span data-v-e475df9d>cnn</span><!----></p><!--]--></a><!----></div><!----></div><!--]--></div></div></section></div><div class="no-transition group" data-v-1935dc3f><section class="vp-sidebar-item sidebar-item level-0 collapsible" data-v-1935dc3f data-v-e475df9d><div class="item" role="button" tabindex="0" data-v-e475df9d><div class="indicator" data-v-e475df9d></div><!----><h2 class="text" data-v-e475df9d><span data-v-e475df9d>概论</span><!----></h2><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-e475df9d><span class="vpi-chevron-right caret-icon" data-v-e475df9d></span></div></div><div data-v-e475df9d data-v-e475df9d><div class="items" data-v-e475df9d><!--[--><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-e475df9d data-v-e475df9d><div class="item" data-v-e475df9d><div class="indicator" data-v-e475df9d></div><!----><a class="vp-link link link" href="/notes/Lee_DL/t9rbwk0z/" data-v-e475df9d><!--[--><p class="text" data-v-e475df9d><span data-v-e475df9d>DL_Note</span><!----></p><!--]--></a><!----></div><!----></div><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-e475df9d data-v-e475df9d><div class="item" data-v-e475df9d><div class="indicator" data-v-e475df9d></div><!----><a class="vp-link link link" href="/notes/Lee_DL/eezwc65m/" data-v-e475df9d><!--[--><p class="text" data-v-e475df9d><span data-v-e475df9d>ML_Note</span><!----></p><!--]--></a><!----></div><!----></div><!--]--></div></div></section></div><!--]--><!--[--><!--]--></nav></aside><!--[--><div id="VPContent" vp-content class="vp-content has-sidebar" data-v-06138826 data-v-30ea5983><div class="vp-doc-container has-sidebar has-aside" data-v-30ea5983 data-v-81bc4f2f><!--[--><!--]--><div class="container" data-v-81bc4f2f><div class="aside" vp-outline data-v-81bc4f2f><div class="aside-curtain" data-v-81bc4f2f></div><div class="aside-container" data-v-81bc4f2f><div class="aside-content" data-v-81bc4f2f><div class="vp-doc-aside" data-v-81bc4f2f data-v-79981e84><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="vp-doc-aside-outline" role="navigation" data-v-79981e84 data-v-6fccab79><div class="content" data-v-6fccab79><div class="outline-marker" data-v-6fccab79></div><div id="doc-outline-aria-label" aria-level="2" class="outline-title" role="heading" data-v-6fccab79><span data-v-6fccab79>此页内容</span><span class="vpi-print icon" data-v-6fccab79></span></div><ul class="root" data-v-6fccab79 data-v-c4088ef2><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-79981e84></div><!--[--><!--]--></div></div></div></div><div class="content" data-v-81bc4f2f><div class="content-container" data-v-81bc4f2f><!--[--><!--]--><main class="main" data-v-81bc4f2f><nav class="vp-breadcrumb" data-v-81bc4f2f data-v-5df90dbf><ol vocab="https://schema.org/" typeof="BreadcrumbList" data-v-5df90dbf><!--[--><li property="itemListElement" typeof="ListItem" data-v-5df90dbf><a class="vp-link link no-icon breadcrumb" href="/" property="item" typeof="WebPage" data-v-5df90dbf><!--[-->首页<!--]--></a><span class="vpi-chevron-right" data-v-5df90dbf></span><meta property="name" content="首页" data-v-5df90dbf><meta property="position" content="1" data-v-5df90dbf></li><li property="itemListElement" typeof="ListItem" data-v-5df90dbf><span class="vp-link no-icon breadcrumb" property="item" typeof="WebPage" data-v-5df90dbf><!--[-->深度学习笔记<!--]--></span><span class="vpi-chevron-right" data-v-5df90dbf></span><meta property="name" content="深度学习笔记" data-v-5df90dbf><meta property="position" content="2" data-v-5df90dbf></li><li property="itemListElement" typeof="ListItem" data-v-5df90dbf><a class="vp-link link no-icon breadcrumb current" href="/notes/Lee_DL/w48llyos/" property="item" typeof="WebPage" data-v-5df90dbf><!--[-->transformer<!--]--></a><!----><meta property="name" content="transformer" data-v-5df90dbf><meta property="position" content="3" data-v-5df90dbf></li><!--]--></ol></nav><!--[--><!--]--><!--[--><div class="vp-doc-title" data-v-e82414c8><!--[--><!--]--><h1 class="page-title" data-v-e82414c8><!----> transformer <!----></h1><!--[--><!--]--></div><div class="vp-doc-meta" data-v-e82414c8><!--[--><!--]--><p class="reading-time" data-v-e82414c8><span class="vpi-books icon" data-v-e82414c8></span><span data-v-e82414c8>约 1321 字</span><span data-v-e82414c8>大约 4 分钟</span></p><!----><!--[--><!--]--><p class="create-time" data-v-e82414c8><span class="vpi-clock icon" data-v-e82414c8></span><span data-v-e82414c8>2025-12-29</span></p></div><!--]--><!--[--><!--]--><!--[--><div class="_notes_Lee_DL_w48llyos_ external-link-icon-enabled vp-doc plume-content" vp-content data-v-81bc4f2f><!--[--><!--]--><div data-v-81bc4f2f><blockquote><p>最早的transformer = encoder + decoder</p></blockquote><h2 id="cnn与nlp的形象类比关系" tabindex="-1"><a class="header-anchor" href="#cnn与nlp的形象类比关系"><span>CNN与NLP的形象类比关系</span></a></h2><p><strong>CNN:</strong></p><p>针对一个图片，可以理解为一张二维的像素图，有多个通道。</p><p><strong>NLP:</strong></p><p>针对一句话，拆分为n个token，每个 token 经过 embedding 后，表示为一个固定维度的向量。</p><p>每个token可以理解为CNN中的一个像素点（贯穿多个通道，通道数即 embedding 维度）。</p><p>所有的token合起来就是一个像素图（贯穿多个通道）。</p><h2 id="layer-normalization" tabindex="-1"><a class="header-anchor" href="#layer-normalization"><span>Layer Normalization</span></a></h2><p>Batch Normalization (BN) 虽然在 CNN 上效果很好，但在以下场景存在致命缺陷，促使了 LN 的诞生：</p><ol><li><strong>RNN/LSTM 等序列模型</strong>：序列长度不固定。如果在不同时间步做 BN，统计量会波动极大且难以维护。</li><li><strong>小 Batch Size (甚至 Batch=1)</strong>：大模型（如 LLM）训练时显存受限，Batch Size 往往很小，此时 BN 的统计量估算失效。</li></ol><p>LN 通过<strong>抛弃对 Batch 维度的依赖</strong>，完美解决了上述问题。</p><h3 id="_1-ln-和-bn-的区别" tabindex="-1"><a class="header-anchor" href="#_1-ln-和-bn-的区别"><span>1. LN 和 BN 的区别</span></a></h3><p>很像，但是：</p><p>假设输入张量形状为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mi>C</mi><mo separator="true">,</mo><mi>H</mi><mo separator="true">,</mo><mi>W</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, C, H, W)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mclose">)</span></span></span></span></p><ul><li><p><strong>BN (纵向切)</strong>：固定 Channel，跨越 Batch (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span>) 统计。</p><p>在cnn中，可以想象成把一个通道的那张图norm。BN 非常适合 CNN。</p></li><li><p><strong>LN (横向切)</strong>：固定 Sample (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span>)，跨越 Feature (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mo separator="true">,</mo><mi>H</mi><mo separator="true">,</mo><mi>W</mi></mrow><annotation encoding="application/x-tex">C, H, W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span></span></span></span>) 统计。</p><p>在cnn中，可以想象成定一个点，然后垂直和每一个通道的该位置点，合起来一起norm。但是 LN <strong>在 CNN 上表现一般</strong>：在图像分类等任务中，LN 往往打不过 BN。</p><ul><li><em>原因</em>：图像的特征（Channel）之间差异很大（比如边缘检测 vs 颜色检测），强制把它们拉到一个分布可能会破坏图像的空间/语义信息。</li></ul></li></ul><h3 id="_2-对于transformer-用ln" tabindex="-1"><a class="header-anchor" href="#_2-对于transformer-用ln"><span>2. 对于Transformer，用LN</span></a></h3><p>原因：</p><ol><li><strong>序列长度不定</strong>：NLP 句子有长有短，BN 需要对齐长度（Padding），Padding 的 0 值会严重干扰 BN 的均值方差计算。</li><li><strong>样本统计不稳</strong>：NLP 数据的 Batch 统计量往往不如图像稳定，且 Batch Size 受显存限制通常较小，BN 效果差。</li></ol><h2 id="encoder" tabindex="-1"><a class="header-anchor" href="#encoder"><span>encoder</span></a></h2><h3 id="bert结构" tabindex="-1"><a class="header-anchor" href="#bert结构"><span>Bert结构</span></a></h3><p><img src="https://cdn.jsdelivr.net/gh/salt235/tuchuang@main/img/QQ_1767016854328.png" alt="早期的一个encoder结构"></p><p>上图其实就是Bert的结构，Bert本质上就是一个编码器。</p><p>输入是四个向量，输出是四个处理后的特征向量（包含了上下文信息，且与x一一对应）。</p><ul><li><p>首先对输入的向量，进行<strong>位置编码</strong>，然后送入一个多头的Attention，让模型在处理当前单词时，能够“关注”到句子里的其他单词。</p></li><li><p>然后<strong>Add</strong>: 指的是 Residual Connection (残差连接)。看旁边的箭头，输入直接绕过了 Attention 层加到了输出上。这能防止网络过深导致的退化。同时<strong>Norm</strong>: 指的就是 <strong>Layer Normalization (LN)</strong>。</p></li><li><p>接着进入一个就是一个简单的<strong>全连接网络（MLP）</strong>，对特征进行进一步的非线性变换。</p></li><li><p>最后再次Add和Norm，输出。</p></li></ul><p>BERT 就是一堆 <strong>Transformer Encoder</strong> 叠起来。每个 Encoder 层里有两步核心操作：先做<strong>注意力 (Attention)</strong>，再做<strong>前馈 (Feed Forward)</strong>，每一步做完都要记得 <strong>Add &amp; Norm (残差+层归一化)</strong>。</p><blockquote><p>在 BERT/Transformer 的图里，“前馈”就是一个<strong>负责特征提取和非线性变换的“加工厂”</strong>。它没有记忆功能（不存状态），只是单纯地把上一层给它的数据处理好，然后扔给下一层。</p></blockquote><h2 id="decoder" tabindex="-1"><a class="header-anchor" href="#decoder"><span>Decoder</span></a></h2><h3 id="decoder的结构" tabindex="-1"><a class="header-anchor" href="#decoder的结构"><span>Decoder的结构</span></a></h3><p><img src="https://cdn.jsdelivr.net/gh/salt235/tuchuang@main/img/QQ_1767949811153.png" alt="decoder"></p><p>Transformer 的 Decoder 是一种用于序列生成的网络结构，<br> 它通过“因果自注意力（Causal Self-Attention）”实现 autoregressive（自回归）建模。</p><h3 id="masked-self-attention" tabindex="-1"><a class="header-anchor" href="#masked-self-attention"><span>Masked Self-attention</span></a></h3><p><img src="https://cdn.jsdelivr.net/gh/salt235/tuchuang@main/img/QQ_1767949111534.png" alt="Masked Self-attention"></p><p>和Self-attention的区别是，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>b</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">b_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>的生成之和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>a</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">a_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>到<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>a</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">a_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>有关，不考虑后面的。</p><h4 id="为什么要masked" tabindex="-1"><a class="header-anchor" href="#为什么要masked"><span>为什么要masked？</span></a></h4><p>因为Decoder的输入不是一下子把所有的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>a</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">a_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>都给出来，而且先<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>a</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">a_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，再一个一个到<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>a</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">a_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。</p><h2 id="encoder和decoder的连接" tabindex="-1"><a class="header-anchor" href="#encoder和decoder的连接"><span>Encoder和Decoder的连接</span></a></h2><p><img src="https://cdn.jsdelivr.net/gh/salt235/tuchuang@main/img/QQ_1767950220351.png" alt="encoder和decoder的连接"></p><h3 id="cross-attention" tabindex="-1"><a class="header-anchor" href="#cross-attention"><span>Cross attention</span></a></h3><p><img src="https://cdn.jsdelivr.net/gh/salt235/tuchuang@main/img/QQ_1767950312059.png" alt="cross attention"></p><p>Cross-Attention 是Decoder 在生成当前 token 时，去“查询（Query）Encoder 输出”的机制。</p><p>Decoder产生Q去查询Encoder的K和。</p><h2 id="seq2seq模型的训练" tabindex="-1"><a class="header-anchor" href="#seq2seq模型的训练"><span>seq2seq模型的训练</span></a></h2><blockquote><p>Seq2Seq（Encoder–Decoder）模型的训练方式是： 用“已知的目标序列”作为 Decoder 的输入， 通过 autoregressive 预测下一个 token， 对每一步计算交叉熵损失，并整体反向传播。</p></blockquote><p>这套方法有个名字：<strong>Teacher Forcing</strong>。</p><h3 id="为什么什么用-已知的目标序列-作为训练decoder的输入呢" tabindex="-1"><a class="header-anchor" href="#为什么什么用-已知的目标序列-作为训练decoder的输入呢"><span>为什么什么用“已知的目标序列”作为训练Decoder的输入呢？</span></a></h3><p>Decoder 训练时输入目标序列，不是让模型“抄答案”，而是让它在“正确历史条件下学习如何预测下一个 token”，这使得序列生成可以并行训练。</p><h3 id="暴露偏差-exposure-bias" tabindex="-1"><a class="header-anchor" href="#暴露偏差-exposure-bias"><span>暴露偏差（Exposure Bias）</span></a></h3><p>模型训练时只“暴露”在真实历史上， 推理时却必须在“自己生成的历史”上继续预测。<strong>明知道有暴露偏差，为什么还要 Teacher Forcing？</strong></p><h4 id="_1-不-teacher-forcing-根本训不动" tabindex="-1"><a class="header-anchor" href="#_1-不-teacher-forcing-根本训不动"><span>1. 不 teacher forcing，根本训不动</span></a></h4><ul><li>初期模型输出几乎是随机</li><li>输入错误上下文 → 梯度噪声巨大</li><li>训练不稳定，甚至发散</li></ul><h4 id="_2-transformer-需要并行训练" tabindex="-1"><a class="header-anchor" href="#_2-transformer-需要并行训练"><span>2. Transformer 需要并行训练</span></a></h4><ul><li>自回归采样：必须一步步生成</li><li>Teacher forcing：<strong>一次 forward 算所有位置</strong></li></ul><h4 id="_3-实际效果好" tabindex="-1"><a class="header-anchor" href="#_3-实际效果好"><span>3. 实际效果好</span></a></h4><ul><li>大模型 + 大数据</li><li>暴露偏差在实践中被“规模”压住了</li></ul></div><!--[--><h2 id="doc-contributors" tabindex="-1"><a href="#doc-contributors" class="header-anchor"><span>贡献者</span></a></h2><div class="vp-contributors"><a href="https://github.com/salt235" target="_blank" rel="noreferrer" class="vp-contributor"><img src="https://avatars.githubusercontent.com/salt235?v=4" alt class="vp-contributor-avatar"><span class="vp-contributor-name">salt235</span></a></div><!--]--><!----><!----><footer class="vp-doc-footer" data-v-81bc4f2f data-v-1b73a5d8><!--[--><!--]--><!----><!----><nav class="prev-next" data-v-1b73a5d8><div class="pager" data-v-1b73a5d8><a class="vp-link link pager-link prev" href="/notes/Lee_DL/1unm9yzt/" data-v-1b73a5d8><!--[--><span class="desc" data-v-1b73a5d8>上一页</span><span class="title" data-v-1b73a5d8><!----><span data-v-1b73a5d8>Self-attention</span></span><!--]--></a></div><div class="pager" data-v-1b73a5d8><a class="vp-link link pager-link next" href="/notes/Lee_DL/528u1pz0/" data-v-1b73a5d8><!--[--><span class="desc" data-v-1b73a5d8>下一页</span><span class="title" data-v-1b73a5d8><!----><span data-v-1b73a5d8>cnn</span></span><!--]--></a></div></nav></footer></div><!--]--></main><!----><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><!--]--><button type="button" class="vp-back-to-top" aria-label="back to top" data-v-06138826 style="display:none;" data-v-95456f92><span class="percent" data-allow-mismatch data-v-95456f92>0%</span><span class="show icon vpi-back-to-top" data-v-95456f92></span><svg aria-hidden="true" data-v-95456f92><circle cx="50%" cy="50%" data-allow-mismatch style="stroke-dasharray:calc(0% - 12.566370614359172px) calc(314.1592653589793% - 12.566370614359172px);" data-v-95456f92></circle></svg></button><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="sign down" class="vp-sign-down" aria-hidden="true" data-v-06138826 style="display:none;" data-v-95c0fd36><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" data-v-95c0fd36><path d="m19 11l-7 6l-7-6" data-v-95c0fd36></path><path d="m19 5l-7 6l-7-6" opacity="0.6" data-v-95c0fd36></path></g></svg><footer class="vp-footer has-sidebar" vp-footer data-v-06138826 data-v-bbedd8e1><!--[--><div class="container" data-v-bbedd8e1><div class="message" data-v-bbedd8e1>Power by <a target="_blank" href="https://v2.vuepress.vuejs.org/">VuePress</a> & <a target="_blank" href="https://theme-plume.vuejs.press">vuepress-theme-plume</a></div><div class="copyright" data-v-bbedd8e1>Copyright © 2025-present yyx235 | <a href="https://beian.miit.gov.cn/" target="_blank">苏ICP备2025228922号-1</a></div></div><!--]--></footer><!--[--><!--]--><!--]--></div><!----><!--]--><!--[--><!--]--><!--]--></div><script type="module" src="/assets/app-DCYTdW2o.js" defer></script></body></html>