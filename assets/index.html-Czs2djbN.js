import{_ as a,c as n,a as t,o as e}from"./app-DCYTdW2o.js";const p={};function i(l,s){return e(),n("div",null,[...s[0]||(s[0]=[t(`<h2 id="第一部分-基础概念-厘清现代ai架构中的-剪枝" tabindex="-1"><a class="header-anchor" href="#第一部分-基础概念-厘清现代ai架构中的-剪枝"><span>第一部分：基础概念：厘清现代AI架构中的“剪枝”</span></a></h2><p>在深入探讨大型语言模型（LLM）的智能剪枝技术之前，必须首先澄清一个关键的概念混淆。当前人工智能领域的发展呈现出一种架构趋同的趋势，特别是Transformer架构在自然语言处理（NLP）和计算机视觉（CV）等多个领域取得了主导地位 1。这种跨领域的成功导致了术语的“语义碰撞”，即同一个术语（如“剪枝”）在不同领域中指代截然不同的技术和数据对象。用户的疑问中将“体素信息（Voxel Information）”与大语言模型的剪枝联系起来，正是这种现象的典型体现。因此，本报告的首要任务是精确区分这两个领域的“剪枝”，为后续的深入讨论奠定坚实而清晰的基础。</p><h3 id="_1-1-作为通用ai策略的剪枝原则" tabindex="-1"><a class="header-anchor" href="#_1-1-作为通用ai策略的剪枝原则"><span>1.1 作为通用AI策略的剪枝原则</span></a></h3><p>剪枝（Pruning）是人工智能和机器学习中一个广泛应用的核心概念，其根本目标是通过移除模型或数据结构中冗余、不重要或贡献较小的部分，来提升计算效率、降低内存占用和加速推理过程 3。这是一种优化思想，其具体实现高度依赖于应用领域和目标对象。例如，在模型压缩领域，剪枝通常指代移除神经网络中的权重或连接。这又可分为非结构化剪枝（移除单个权重）和结构化剪枝（移除整个神经元、通道甚至层），后者因其对硬件更友好而备受关注 3。将智能剪枝置于这一宏观背景下，有助于理解其本质——一种为特定目标（LLM推理效率）而设计的、高度特化的剪枝应用。</p><h3 id="_1-2-领域一-3d计算机视觉中的体素剪枝" tabindex="-1"><a class="header-anchor" href="#_1-2-领域一-3d计算机视觉中的体素剪枝"><span>1.2 领域一：3D计算机视觉中的体素剪枝</span></a></h3><p>为了彻底解决用户的困惑，我们首先剖析3D计算机视觉领域的“体素剪枝”。</p><p>体素（Voxel）的定义与作用</p><p>体素，即“体积像素”（Volumetric Pixel），是3D空间网格化表示的基本单位，可以类比为2D图像中的像素 4。在3D目标检测、场景重建、医学成像等任务中，点云等非结构化3D数据常被转换成规则的体素网格，以便利用成熟的卷积神经网络（CNN）或Transformer架构进行处理 1。</p><p>3D视觉中的Transformer与体素剪枝</p><p>近年来，Voxel Transformer (VoTr)、VoxelFormer等模型将Transformer架构成功应用于3D数据处理，通过自注意力机制捕捉3D场景中不同区域（体素）之间的长距离依赖关系 1。然而，3D场景通常是高度稀疏的，绝大部分空间是空的，这意味着大量的体素不包含任何有效信息。直接在完整的体素网格上运行计算会造成巨大的资源浪费。“体素剪枝”正是在此背景下产生的，其核心思想是智能地识别并移除这些空的或与任务无关的体素，从而使后续的计算（无论是3D卷积还是Transformer的自注意力）只在包含有效信息的稀疏体素上进行，极大地降低了计算和内存开销 8。</p><p>具体案例：文本引导的稀疏体素剪枝（TSP3D）</p><p>TSP3D 10 是一个绝佳的例证。在“3D视觉定位”（3D Visual Grounding）任务中，模型需要根据一段自然语言描述在3D场景中定位一个物体。TSP3D利用文本描述作为引导，迭代地剪枝场景的体素表示。它会逐步移除背景和与文本描述无关物体的体素，将计算资源集中在可能的目标物体周围。这种剪枝策略是针对多模态交互的效率优化，其操作对象是3D空间的几何表示。</p><p>结论：与LLM剪枝的根本区别</p><p>至此可以明确，当一个标准的、基于文本的大语言模型进行推理时，它不会获取或使用任何“体素信息”。体素剪枝与LLM的KV缓存剪枝是两个应用于完全不同数据类型和维度的独立概念：</p><ul><li><strong>体素剪枝</strong>：操作对象是<strong>三维空间数据结构</strong>（体素网格），目的是降低3D场景表示的复杂性。</li><li><strong>KV缓存剪枝</strong>：操作对象是<strong>一维序列/时间数据结构</strong>（Token缓存），目的是缩减上下文历史的长度。</li></ul><h3 id="_1-3-领域二-大型语言模型中的kv缓存剪枝" tabindex="-1"><a class="header-anchor" href="#_1-3-领域二-大型语言模型中的kv缓存剪枝"><span>1.3 领域二：大型语言模型中的KV缓存剪枝</span></a></h3><p>现在，我们将焦点转向本报告的核心主题。在大型语言模型的语境下，“智能剪枝”几乎完全是指对<strong>键值缓存（Key-Value Cache, KV Cache）</strong> 进行选择性数据移除的过程。</p><p>KV缓存是一种在LLM自回归生成过程中用于存储先前处理过的所有Token的中间注意力状态（即“键”和“值”向量）的内存区域 13。随着生成序列的增长，KV缓存的体积也随之线性增加，迅速成为内存瓶颈。KV缓存剪枝的核心任务，就是设计一种策略，判断哪些历史Token对于预测下一个Token“不那么重要”，然后将其对应的键值对从缓存中丢弃，从而在有限的内存预算内维持尽可能长的有效上下文 15。这正是我们接下来将要深入探讨的“智能剪枝”。</p><h2 id="第二部分-kv缓存-自回归生成的记忆支柱" tabindex="-1"><a class="header-anchor" href="#第二部分-kv缓存-自回归生成的记忆支柱"><span>第二部分：KV缓存：自回归生成的记忆支柱</span></a></h2><p>要理解为何需要对KV缓存进行剪枝，首先必须深入理解其在大型语言模型推理过程中的核心作用、其不可或缺性，以及它如何从一个优化手段演变为性能瓶颈。这一演变过程本身就揭示了复杂系统中优化策略的内在循环：一个为解决旧问题而生的方案，其成功本身又催生了新的挑战。</p><h3 id="_2-1-transformer中的自回归生成" tabindex="-1"><a class="header-anchor" href="#_2-1-transformer中的自回归生成"><span>2.1 Transformer中的自回归生成</span></a></h3><p>以GPT系列为代表的解码器-纯架构（Decoder-Only）模型，其文本生成过程是<strong>自回归</strong>的 17。这意味着模型逐个Token地生成文本，每个新生成的Token都基于所有先前输入和已生成的Token序列作为上下文来进行预测。这个过程的核心是自注意力（Self-Attention）机制。对于序列中的每一个Token，模型都会计算出三个向量：查询（Query, Q）、键（Key, K）和值（Value, V）。通过计算当前Token的Q向量与所有历史Token的K向量的点积，模型可以得到一个注意力分数分布，该分数决定了在生成下一个Token时，应该对每个历史Token赋予多大的“关注权重”。这些权重随后被用于对所有历史Token的V向量进行加权求和，形成当前步骤的输出 17。</p><h3 id="_2-2-kv缓存的诞生-一项必要的优化" tabindex="-1"><a class="header-anchor" href="#_2-2-kv缓存的诞生-一项必要的优化"><span>2.2 KV缓存的诞生：一项必要的优化</span></a></h3><p>冗余计算的问题</p><p>在没有缓存的朴素自回归推理中，每生成一个新Token（例如第 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6151em;"></span><span class="mord mathnormal">t</span></span></span></span> 个Token），模型都需要重新计算从第1个到第 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">t-1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6984em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span> 个所有历史Token的K和V向量。这意味着，生成第100个Token时，需要为前99个Token计算99对K/V向量；生成第101个Token时，又需要为前100个Token重新计算100对K/V向量。这种计算方式的复杂度随序列长度呈二次方增长（<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>），对于长序列生成而言，其计算成本是无法接受的 18。</p><p>解决方案：缓存键与值</p><p>KV缓存正是为了解决这一一级问题而设计的。其原理非常直观：当一个Token的K和V向量被计算出来后，就将它们存储起来。在生成下一个Token时，模型无需再为历史Token重新计算，只需计算当前新Token自身的Q、K、V向量，然后从缓存中直接读取所有历史Token的K和V向量，即可执行注意力计算 14。这一机制将每步生成的计算复杂度从依赖于整个序列长度降低到了一个常数级别，极大地加速了推理过程。这是一种典型的以空间换时间（使用更多内存以节省计算时间）的优化策略 17。</p><h3 id="_2-3-llm推理的两个阶段-prefill与decode" tabindex="-1"><a class="header-anchor" href="#_2-3-llm推理的两个阶段-prefill与decode"><span>2.3 LLM推理的两个阶段：Prefill与Decode</span></a></h3><p>LLM的推理过程通常分为两个截然不同的阶段，理解这两个阶段对于后续讨论剪枝策略至关重要。</p><p>Prefill阶段（提示处理）</p><p>这是推理的第一步，模型接收用户的完整输入提示（Prompt），并一次性地、并行地处理所有Token。这个阶段的主要任务是为整个提示序列生成初始的KV缓存。由于是并行计算，其瓶颈通常在于内存带宽。对于非常长的输入提示，即便是Prefill阶段也可能耗尽内存，因此一些先进的剪枝策略也会在这一阶段介入 13。</p><p>Decode阶段（Token生成）</p><p>在Prefill阶段结束后，模型进入逐个Token生成的自回归循环。每一步，模型利用完整的KV缓存（包含提示和之前已生成的所有Token）来预测下一个Token，然后将新生成的Token的K/V向量追加到缓存中。这个阶段是序列化的，其瓶颈在于每一步的计算延迟。正是在这个阶段，KV缓存的持续增长使其内存消耗问题变得尤为突出 13。</p><h3 id="_2-4-kv缓存成为瓶颈" tabindex="-1"><a class="header-anchor" href="#_2-4-kv缓存成为瓶颈"><span>2.4 KV缓存成为瓶颈</span></a></h3><p>KV缓存的成功引入解决了计算冗余的一级问题，但其自身也带来了新的、二级的挑战：内存消耗。</p><p>量化内存成本</p><p>KV缓存的大小可以通过以下公式进行估算：</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>Size</mtext><mo>=</mo><mi>B</mi><mo>×</mo><mi>L</mi><mo>×</mo><mi>H</mi><mo>×</mo><mi>N</mi><mo>×</mo><mi>d</mi><mo>×</mo><mi>P</mi></mrow><annotation encoding="application/x-tex">\\text{Size} = B \\times L \\times H \\times N \\times d \\times P </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord text"><span class="mord">Size</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">L</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span></span></span></span></span></p><p>其中，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi></mrow><annotation encoding="application/x-tex">B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span></span></span></span> 是批量大小（Batch Size），<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi></mrow><annotation encoding="application/x-tex">L</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">L</span></span></span></span> 是序列长度，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi></mrow><annotation encoding="application/x-tex">H</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span></span></span></span> 是Transformer层数，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span> 是注意力头数，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">d</span></span></span></span> 是每个头的维度，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi></mrow><annotation encoding="application/x-tex">P</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span></span></span></span> 是数据精度（例如FP16为2字节）。从公式中可以清晰地看到，缓存大小与序列长度 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi></mrow><annotation encoding="application/x-tex">L</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">L</span></span></span></span> 线性相关 13。</p><p>实际影响</p><p>对于需要处理长文本（如长文档问答、摘要）或需要高吞吐量（大批量处理）的应用场景，KV缓存的体积会急剧膨胀，甚至可能超过模型权重本身占用的内存 13。这直接导致了几个严重的实际问题：</p><ol><li><strong>限制了最大上下文长度</strong>：一旦KV缓存耗尽可用显存，模型便无法处理更长的序列。</li><li><strong>降低了吞吐量</strong>：为了在有限的显存中运行，必须减小批量大小，这直接损害了系统的整体处理能力。</li><li><strong>提高了硬件门槛</strong>：需要配备更大显存的昂贵硬件才能支持长上下文应用。</li></ol><p>因此，管理和压缩KV缓存，已成为部署和优化大型语言模型的关键所在。智能剪枝技术，正是为了应对这一由KV缓存的成功所催生的二级问题而发展起来的核心解决方案之一。</p><h2 id="第三部分-智能剪枝的机制" tabindex="-1"><a class="header-anchor" href="#第三部分-智能剪枝的机制"><span>第三部分：智能剪枝的机制</span></a></h2><p>本节将直接回答用户关于智能剪枝的核心问题：它在<strong>何时</strong>执行，剪枝的<strong>对象是什么</strong>，以及<strong>如何</strong>决定剪枝哪些内容。我们将通过一个结构化的分类法，从简单的启发式方法到更复杂的、有理论依据的策略，系统地梳理剪枝的执行机制。</p><h3 id="_3-1-剪枝的执行时机" tabindex="-1"><a class="header-anchor" href="#_3-1-剪枝的执行时机"><span>3.1 剪枝的执行时机</span></a></h3><p>智能剪枝是一个在模型推理过程中动态执行的过程。其主要触发时机是在<strong>Decode阶段</strong>。每当一个新的Token生成并其对应的KV对被添加到缓存后，系统会检查当前的缓存大小是否超出了预设的内存预算。如果超出，剪枝算法就会被激活，以驱逐（Evict）一部分现有的Token，为后续生成腾出空间 13。</p><p>此外，对于处理极长输入提示的场景，一些先进的策略（如PagedEviction）也会在<strong>Prefill阶段</strong>执行某种形式的剪枝或选择。这确保了在进入Decode阶段之前，初始的KV缓存状态就已经符合内存预算，避免了一开始就因提示过长而导致内存溢出 13。</p><h3 id="_3-2-剪枝的对象与粒度" tabindex="-1"><a class="header-anchor" href="#_3-2-剪枝的对象与粒度"><span>3.2 剪枝的对象与粒度</span></a></h3><p>剪枝的直接操作对象是存储在缓存中的<strong>键值对（Key-Value Pairs）</strong>。当一个Token被选中进行驱逐时，其在所有注意力层、所有注意力头中对应的K和V向量都会被从缓存中移除。根据移除操作的组织方式，剪枝可以有不同的粒度。</p><ul><li><strong>Token级粒度</strong>：这是最直接的方式，算法为每个单独的Token计算一个重要性分数，然后驱逐分数最低的Token。</li><li><strong>结构化/块级粒度</strong>：在像vLLM这样采用PagedAttention内存管理技术的系统中，KV缓存被组织成固定大小的内存块（Pages）13。在这种架构下，逐个Token地驱逐会导致内存碎片化且管理开销高。因此，更高效的做法是进行块级驱逐，即一次性驱逐一整个内存块。一个块是否被驱逐，通常取决于其内部所有Token的聚合重要性分数。这种与底层内存布局对齐的剪枝方式，体现了算法与系统协同设计的重要性。</li><li><strong>通道级粒度</strong>：这是一个相关但不同的概念。像KVPruner这样的方法，执行的是对K和V向量内部特征通道的结构化剪枝，旨在永久性地减小每个Token的KV向量维度（即<code>head_dim</code>）3。这通常是一种静态的模型压缩技术，需要微调，而非动态的Token驱逐策略。</li></ul><h3 id="_3-3-如何确定重要性-显著性度量分类法" tabindex="-1"><a class="header-anchor" href="#_3-3-如何确定重要性-显著性度量分类法"><span>3.3 如何确定重要性：显著性度量分类法</span></a></h3><p>决定<strong>哪些</strong>Token应该被剪枝是“智能”剪枝的核心。这个决策过程依赖于一个显著性度量（Saliency Metric）来为每个Token或Token块打分。这些度量方法的发展历程，反映了研究者们从简单的启发式规则到复杂的、理论驱动的分析方法的演进。</p><h4 id="_3-3-1-基于启发式的方法" tabindex="-1"><a class="header-anchor" href="#_3-3-1-基于启发式的方法"><span>3.3.1 基于启发式的方法</span></a></h4><p>这类方法依赖于一些直观的、计算成本低的规则。</p><ul><li><strong>基于近因性（Recency-Based）</strong>：最简单的方法是保留一个固定大小的近期Token窗口，即<strong>滑动窗口（Sliding Window）</strong>。一旦窗口满了，最老的Token就会被驱逐。<strong>StreamingLLM</strong> 13 对此进行了改进，它发现序列最初的几个Token（被称为“注意力汇点”，Attention Sinks）对于维持语言模型的注意力结构至关重要，因此它会永久保留这些初始Token，同时维护一个滑动的近期Token窗口。</li></ul><h4 id="_3-3-2-注意力感知方法-及其代理" tabindex="-1"><a class="header-anchor" href="#_3-3-2-注意力感知方法-及其代理"><span>3.3.2 注意力感知方法（及其代理）</span></a></h4><p>理论上，一个Token的重要性最直接的体现就是它在注意力计算中获得的分数。然而，在实际部署中，获取这些分数面临着系统层面的挑战。</p><ul><li><strong>直接注意力分数</strong>：一个Token被后续Token关注的越多（即获得的注意力权重越高），它就越重要。然而，为了极致的计算效率，现代的注意力实现（如<strong>FlashAttention</strong>）通过融合计算内核、避免实例化完整的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>×</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">N \\times N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span> 注意力矩阵来加速计算和节省内存 13。这一关键的系统级优化，使得在推理过程中无法直接获取到完整的注意力分数矩阵，从而阻碍了基于它的剪枝方法。</li><li><strong>代理度量（Proxy Metrics）</strong>：这种系统约束催生了寻找注意力分数代理指标的研究。既然无法获得最终的注意力权重，研究者们转而寻找与重要性高度相关且易于计算的替代品。 <ul><li><strong>Key向量的L2范数</strong>：一个Key向量的欧几里得长度（L2范数）在一定程度上可以反映其在注意力计算中的“能量”或显著性，被用作一种有效的代理指标 16。</li><li><strong>其他代理</strong>：还包括Key向量与后续Query向量点积的方差等，这些指标试图从不同角度间接衡量一个Token的潜在影响力。</li></ul></li></ul><h4 id="_3-3-3-基于扰动的显著性" tabindex="-1"><a class="header-anchor" href="#_3-3-3-基于扰动的显著性"><span>3.3.3 基于扰动的显著性</span></a></h4><p>这类方法代表了更为严谨和理论化的尝试，它们不依赖于代理指标，而是直接估算剪枝操作对模型输出的影响。</p><ul><li><strong>Optimal Brain Cache (OBCache)</strong>：该方法根植于经典的“最优脑损伤”（Optimal Brain Damage）理论 15。它不问一个Token的注意力分数有多高，而是问“如果我剪掉这个Token，模型的注意力输出会产生多大的误差（扰动）？”。OBCache通过二阶泰勒展开，推导出了一个用于估算这种扰动的闭式解，从而可以高效地计算每个Token的显著性分数。其核心创新在于，这种分数是“输出感知”（Output-aware）的，它直接量化了剪枝对模型计算流的实际影响，而不仅仅是依赖于注意力权重等中间产物 15。</li></ul><h4 id="_3-3-4-基于模型信息的度量" tabindex="-1"><a class="header-anchor" href="#_3-3-4-基于模型信息的度量"><span>3.3.4 基于模型信息的度量</span></a></h4><p>这类方法通常用于静态剪枝，通过在校准数据集上评估剪枝对模型整体性能的影响来指导剪枝策略。</p><ul><li><strong>基于困惑度（Perplexity, PPL）</strong>：以KVPruner为例，该方法通过评估剪枝模型不同部分（如整个Transformer块）对模型在某个数据集上困惑度的影响来判断其重要性。那些剪枝后导致困惑度大幅上升的模块被认为是更敏感、更重要的，因此会分配较少的剪枝比例 3。</li></ul><p>这种从简单启发式到系统感知代理，再到理论驱动的扰动分析的演进路径，清晰地展示了智能剪枝领域在追求更高精度和更强实用性之间不断权衡与创新的过程。特别是，对FlashAttention等底层优化的适应性，已成为衡量现代剪枝算法实用价值的关键标准。</p><h2 id="第四部分-算法分析-剪枝策略分步详解" tabindex="-1"><a class="header-anchor" href="#第四部分-算法分析-剪枝策略分步详解"><span>第四部分：算法分析：剪枝策略分步详解</span></a></h2><p>为了具体地阐释智能剪枝的运作流程，本节将详细剖析一个代表性的算法。由于原始论文中的“算法一”未知，我们将基于现代系统感知剪枝技术的通用原则，特别是PagedEviction 13 中体现的思想，构建一个名为“页面感知的按重要性剪枝”（Paged-Aware Importance Pruning）的规范化算法。这个算法旨在提供一个具体、实用且与现代LLM服务框架（如vLLM）兼容的示例。</p><h3 id="_4-1-算法前言" tabindex="-1"><a class="header-anchor" href="#_4-1-算法前言"><span>4.1 算法前言</span></a></h3><p>需要说明的是，以下呈现的算法是一个综合性的示例，旨在教学和阐明核心逻辑。它融合了块级驱逐、近期Token保护和基于代理指标的重要性计算等关键概念，这些都是当前先进剪枝策略的共同特征。</p><h3 id="_4-2-表1-算法符号词汇表" tabindex="-1"><a class="header-anchor" href="#_4-2-表1-算法符号词汇表"><span>4.2 表1：算法符号词汇表</span></a></h3><p>在展示伪代码之前，我们首先定义算法中使用的所有符号，以确保后续解释的清晰性。</p><table><thead><tr><th><strong>符号</strong></th><th><strong>定义</strong></th><th><strong>示例值</strong></th></tr></thead><tbody><tr><td><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span></span></span></span></td><td>完整的KV缓存，组织为块的列表</td><td><code>[block_1, block_2,...]</code></td></tr><tr><td><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">b</span></span></span></span></td><td>缓存中的单个内存块</td><td><code>block_i</code></td></tr><tr><td><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>T</mi><mi>b</mi></msub></mrow><annotation encoding="application/x-tex">T_b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">b</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></td><td>存储在块 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">b</span></span></span></span> 中的Token集合</td><td><code>{token_512,..., token_575}</code></td></tr><tr><td><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>k</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>v</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">k_i, v_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0315em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></td><td>Token <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em;"></span><span class="mord mathnormal">i</span></span></span></span> 的键（Key）和值（Value）向量</td><td>形状为 <code>[d_head]</code> 的张量</td></tr><tr><td><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">I(t_i)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></td><td>Token <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em;"></span><span class="mord mathnormal">i</span></span></span></span> 的重要性分数</td><td><code>0.85</code></td></tr><tr><td><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>I</mi><mrow><mi>a</mi><mi>g</mi><mi>g</mi></mrow></msub><mo stretchy="false">(</mo><mi>b</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">I_{agg}(b)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">gg</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">b</span><span class="mclose">)</span></span></span></span></td><td>块 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">b</span></span></span></span> 的聚合重要性分数</td><td><code>0.72</code></td></tr><tr><td><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>B</mi><mrow><mi>b</mi><mi>u</mi><mi>d</mi><mi>g</mi><mi>e</mi><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">B_{budget}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0502em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">b</span><span class="mord mathnormal mtight">u</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">g</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span></td><td>缓存中允许的最大块数</td><td><code>16</code></td></tr><tr><td><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>N</mi><mrow><mi>b</mi><mi>l</mi><mi>o</mi><mi>c</mi><mi>k</mi><mi>s</mi></mrow></msub></mrow><annotation encoding="application/x-tex">N_{blocks}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.109em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">b</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span><span class="mord mathnormal mtight">oc</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mord mathnormal mtight">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></td><td>缓存中当前的块数</td><td><code>17</code></td></tr><tr><td><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>b</mi><mrow><mi>r</mi><mi>e</mi><mi>c</mi><mi>e</mi><mi>n</mi><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">b_{recent}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">rece</span><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></td><td>最近添加的块，该块永远不会被驱逐</td><td><code>block_17</code></td></tr></tbody></table><h3 id="_4-3-页面感知的按重要性剪枝伪代码" tabindex="-1"><a class="header-anchor" href="#_4-3-页面感知的按重要性剪枝伪代码"><span>4.3 页面感知的按重要性剪枝伪代码</span></a></h3><p>以下算法描述了在Decode阶段，每生成一个新Token并将其KV对添加到缓存<strong>之后</strong>所执行的逻辑。</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212;"><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code class="language-"><span class="line"><span>算法 1: 页面感知的按重要性剪枝 (Decode步骤)</span></span>
<span class="line"><span>输入: 缓存 C, 预算 B_budget</span></span>
<span class="line"><span></span></span>
<span class="line"><span>1: function ON_NEW_TOKEN(C, B_budget):</span></span>
<span class="line"><span>2:      N_blocks = len(C)</span></span>
<span class="line"><span>3:      if N_blocks &lt;= B_budget:</span></span>
<span class="line"><span>4:          return C  // 无需驱逐</span></span>
<span class="line"><span></span></span>
<span class="line"><span>5:      // 识别可驱逐的块</span></span>
<span class="line"><span>6:      b_recent = C[-1]</span></span>
<span class="line"><span>7:      EvictableBlocks = C[:-1]</span></span>
<span class="line"><span></span></span>
<span class="line"><span>8:      // 为所有可驱逐块中的Token计算重要性</span></span>
<span class="line"><span>9:      for b in EvictableBlocks:</span></span>
<span class="line"><span>10:         TokenScores =</span></span>
<span class="line"><span>11:         for t_i in T_b:</span></span>
<span class="line"><span>12:             // 使用Key向量的L2范数作为重要性代理</span></span>
<span class="line"><span>13:             score = ||k_i||₂</span></span>
<span class="line"><span>14:             TokenScores.append(score)</span></span>
<span class="line"><span>15:</span></span>
<span class="line"><span>16:         // 聚合块的重要性 (例如，使用平均值)</span></span>
<span class="line"><span>17:         I_agg(b) = mean(TokenScores)</span></span>
<span class="line"><span>18:</span></span>
<span class="line"><span>19:     // 找到聚合重要性最低的块</span></span>
<span class="line"><span>20:     b_to_evict = argmin(I_agg(b) for b in EvictableBlocks)</span></span>
<span class="line"><span>21:</span></span>
<span class="line"><span>22:     // 驱逐最不重要的块</span></span>
<span class="line"><span>23:     C.remove(b_to_evict)</span></span>
<span class="line"><span>24:     FreeMemory(b_to_evict)</span></span>
<span class="line"><span>25:</span></span>
<span class="line"><span>26:     return C</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_4-4-逐行代码解析" tabindex="-1"><a class="header-anchor" href="#_4-4-逐行代码解析"><span>4.4 逐行代码解析</span></a></h3><p>本小节将对上述伪代码进行详细的叙述性解释，阐明每一行或代码块背后的逻辑和意图。</p><ul><li><p>第1-4行 (触发条件)：</p><p>此函数在每个Decode步骤结束时被调用。核心的驱逐逻辑仅在当前缓存的块数 N_blocks 超过预设的预算 B_budget 时才会执行。这是启动驱逐机制的基本前提。如果缓存未满，则直接返回，不执行任何操作。</p></li><li><p>第5-7行 (确定候选对象)：</p><p>这里的逻辑至关重要，体现了一种关键的启发式策略：保护最新生成的Token。b_recent 指的是当前正在被写入的、包含最新Token的内存块。由于新生成的Token与紧随其后的未来Token具有最强的上下文关联性，驱逐这个块是极不合理的。因此，算法将除了这个最新块之外的所有块 C[:-1] 都视为可驱逐的候选对象 EvictableBlocks。这一策略与 13 中描述的原则一致。</p></li><li><p>第8-14行 (逐Token重要性计算)：</p><p>这是算法的“智能”核心所在。代码遍历每一个可驱逐的块，并对块内的每一个Token计算其重要性分数。</p><ul><li><strong>第13行</strong>：这里具体实现了重要性度量。我们采用了第三部分讨论过的一种高效代理：计算每个Token的Key向量的L2范数 <code>||k_i||₂</code> 16。这个值被用作该Token重要性的量化表示。</li></ul></li><li><p>第16-17行 (块级重要性聚合)：</p><p>由于我们的目标是进行块级驱逐以提高效率和避免内存碎片，因此需要将块内所有Token的个体分数聚合成一个能代表整个块重要性的单一分数。</p><ul><li><strong>第17行</strong>：此处我们使用 <code>mean</code>（平均值）作为聚合策略。这意味着一个块的整体重要性被定义为其内部所有Token重要性的平均水平。其他可能的聚合函数还包括 <code>min</code>（最不重要的Token决定了整个块的命运）、<code>max</code> 或加权平均等。这一步是实现 13 中提到的“结构化块级”驱逐的关键。</li></ul></li><li><p>第19-20行 (选择牺牲品)：</p><p>这一步非常直接。算法在所有可驱逐的块中，通过 argmin 操作找到那个具有最低聚合重要性分数 I_agg(b) 的块。这个块 b_to_evict 就是本次驱逐操作的目标。</p></li><li><p>第22-26行 (执行驱逐)：</p><p>最后，算法执行实际的清理工作。</p><ul><li><strong>第23行</strong>：将选定的块从缓存的逻辑数据结构（如列表）中移除。</li><li><strong>第24行</strong>：调用底层函数 <code>FreeMemory</code>，释放该块所占用的物理内存，使其可以被重新分配给未来的Token。</li><li><strong>第26行</strong>：返回更新后的、大小符合预算的缓存，准备进行下一个Token的生成。</li></ul></li></ul><p>通过这个分步解析，我们可以看到智能剪枝算法如何将高级的机器学习概念（如重要性度量）与底层的系统工程考量（如块级内存管理）紧密结合，从而在实际应用中实现高效且有效的KV缓存管理。</p><h2 id="第五部分-kv缓存压缩技术生态系统" tabindex="-1"><a class="header-anchor" href="#第五部分-kv缓存压缩技术生态系统"><span>第五部分：KV缓存压缩技术生态系统</span></a></h2><p>虽然智能剪枝是一种强大且灵活的KV缓存管理技术，但它并非孤立存在。实际上，它是一个更广泛的KV缓存压缩技术生态系统中的一员。理解这个生态系统至关重要，因为它揭示了不同的优化策略如何从不同维度解决内存瓶颈问题，以及它们之间潜在的协同作用。未来的LLM推理优化，很可能不是依赖单一的“银弹”技术，而是通过巧妙地组合这些正交的方法，构建出一个复合式的、多层次的优化堆栈。</p><h3 id="_5-1-一个多维度的问题" tabindex="-1"><a class="header-anchor" href="#_5-1-一个多维度的问题"><span>5.1 一个多维度的问题</span></a></h3><p>我们可以将整个KV缓存想象成一个多维张量，其维度包括批量大小、序列长度、层数、头数、头维度和数据精度。不同的压缩技术正是通过在这些维度上做文章来实现内存节省的。由于它们的攻击目标不同，这些技术在很大程度上是<strong>正交的（Orthogonal）</strong>，意味着它们可以被结合使用以达到累加甚至相乘的优化效果。</p><h3 id="_5-2-替代与互补技术" tabindex="-1"><a class="header-anchor" href="#_5-2-替代与互补技术"><span>5.2 替代与互补技术</span></a></h3><h4 id="_5-2-1-kv量化-kv-quantization" tabindex="-1"><a class="header-anchor" href="#_5-2-1-kv量化-kv-quantization"><span>5.2.1 KV量化 (KV Quantization)</span></a></h4><ul><li><strong>原理</strong>：量化技术的目标是<strong>数据精度</strong>维度。它通过降低存储K和V向量中数值的精度来减小内存占用，例如，将标准的16位浮点数（FP16）转换为8位整数（INT8）甚至4位整数（INT4）16。</li><li><strong>方法</strong>：存在多种量化策略，如FlexGen中使用的分组量化（Group-wise Quantization），或KIVI中对Key和Value采用不同策略（如Per-channel和Per-token）的混合精度量化 16。</li><li><strong>混合方法 - 量化剪枝 (Quantized Pruning)</strong>：16 中提出了一种新颖的混合思想。它挑战了传统剪枝“非黑即白”（保留或丢弃）的二元选择。量化剪枝认为，与其彻底丢弃一个不太重要的Token，不如用更低的精度（如INT4）来存储它。这样，可以在相同的内存预算下，保留更多的上下文信息，尽管部分信息的保真度有所降低。这体现了在上下文<strong>数量</strong>和<strong>质量</strong>之间进行更精细权衡的思路。</li></ul><h4 id="_5-2-2-低秩近似-low-rank-approximation" tabindex="-1"><a class="header-anchor" href="#_5-2-2-低秩近似-low-rank-approximation"><span>5.2.2 低秩近似 (Low-Rank Approximation)</span></a></h4><ul><li><strong>原理</strong>：以LESS方法为例，这类技术主要针对<strong>序列长度</strong>维度，但采用与剪枝不同的策略。它不再将所有历史Token的KV对进行拼接存储，而是维护一个固定大小的、压缩的“状态”矩阵。这个状态矩阵被设计为对整个KV缓存历史的低秩近似。每当有新的KV对生成时，它不是被追加到缓存中，而是被用来以一种递归的方式更新这个低秩状态 19。</li><li><strong>优势</strong>：这种方法的最大优点是实现了<strong>相对于序列长度的恒定内存占用</strong>。无论生成多长的序列，其缓存大小都保持不变，这从根本上解决了KV缓存随序列线性增长的问题 19。</li></ul><h4 id="_5-2-3-架构修改-architectural-modifications" tabindex="-1"><a class="header-anchor" href="#_5-2-3-架构修改-architectural-modifications"><span>5.2.3 架构修改 (Architectural Modifications)</span></a></h4><ul><li><strong>原理</strong>：这类方法从模型结构本身入手，旨在从源头上减少生成KV缓存的大小。它们通常需要对模型进行重新训练或至少是大量的微调。</li><li><strong>方法</strong>： <ul><li><strong>多查询注意力 (MQA) 和分组查询注意力 (GQA)</strong>：这是最成功的架构修改之一。在标准的多头注意力中，每个查询头（Query Head）都对应一组独立的键/值头（Key/Value Head）。而在MQA中，所有的查询头共享同一组K/V头；GQA则是介于两者之间的折中，让一组查询头共享一组K/V头 21。这两种方法都能显著减少需要缓存的K和V向量的数量（即在<strong>头数</strong>维度上进行压缩），同时对模型性能的影响非常小 16。</li><li><strong>跨层共享 (Layer Sharing)</strong>：一些研究探索在相邻的Transformer层之间共享KV缓存，进一步减少冗余存储 16。</li></ul></li></ul><h3 id="_5-3-表2-kv缓存优化技术对比框架" tabindex="-1"><a class="header-anchor" href="#_5-3-表2-kv缓存优化技术对比框架"><span>5.3 表2：KV缓存优化技术对比框架</span></a></h3><p>为了直观地比较这些技术，下表从多个关键维度对它们进行了总结。对于需要部署LLM的工程师或研究人员来说，这个表格可以作为一个快速决策指南，帮助他们根据具体的应用场景和约束条件（例如，是否可以重新训练模型）来选择合适的技术组合。</p><table><thead><tr><th><strong>技术类别</strong></th><th><strong>主要作用维度</strong></th><th><strong>内存节省潜力</strong></th><th><strong>精度影响</strong></th><th><strong>计算开销</strong></th><th><strong>是否需重新训练</strong></th><th><strong>关键文献</strong></th></tr></thead><tbody><tr><td><strong>智能剪枝</strong></td><td>序列长度</td><td>高 (可调)</td><td>中等 (依赖任务)</td><td>低-中 (计算分数)</td><td>否</td><td>[13, 15, 16]</td></tr><tr><td><strong>量化</strong></td><td>数据精度</td><td>中-高</td><td>低-中</td><td>低 (量化/反量化)</td><td>否(PTQ) / 是(QAT)</td><td>16</td></tr><tr><td><strong>低秩近似</strong></td><td>序列长度</td><td>非常高 (恒定)</td><td>中-高</td><td>中 (更新状态)</td><td>否 (基于适配器)</td><td>19</td></tr><tr><td><strong>架构修改 (GQA/MQA)</strong></td><td>注意力头数</td><td>高 (固定)</td><td>非常低</td><td>无 (推理时)</td><td>是 (从头开始)</td><td>16</td></tr></tbody></table><p>这个比较框架清晰地揭示了不同技术之间的权衡。例如，架构修改虽然效果好且无额外推理开销，但灵活性差，需要前期巨大的训练投入。而智能剪枝和量化则属于“免训练”或“轻量微调”方案，可以方便地应用于已有的预训练模型，提供了极大的灵活性。这种多维度的视角表明，LLM推理优化的挑战正从“发明最好的单一算法”转向“设计最优的算法组合”，这是一个更复杂但潜力也更大的系统工程问题。</p><h2 id="第六部分-综合与结论" tabindex="-1"><a class="header-anchor" href="#第六部分-综合与结论"><span>第六部分：综合与结论</span></a></h2><p>本报告系统地剖析了大型语言模型中的智能剪枝技术，旨在为研究人员和工程师提供一份全面而深入的参考。通过澄清基本概念、解构核心机制、分析具体算法，并将其置于更广阔的优化技术生态中，我们得以形成对这一关键领域的 nuanced 理解。</p><h3 id="_6-1-核心学习要点总结" tabindex="-1"><a class="header-anchor" href="#_6-1-核心学习要点总结"><span>6.1 核心学习要点总结</span></a></h3><ol><li><strong>概念的精确性至关重要</strong>：我们首先明确区分了3D计算机视觉中的“体素剪枝”与大型语言模型中的“KV缓存剪枝”。前者处理空间数据，后者处理序列数据。这种区分是理解后续所有讨论的基础。</li><li><strong>KV缓存的双重角色</strong>：KV缓存是加速自回归生成的关键机制，它将推理过程的计算复杂度从二次方降低到线性。然而，其自身随序列长度线性增长的内存消耗，使其成为长上下文应用中的主要性能瓶LEI 13。</li><li><strong>智能剪枝的本质</strong>：智能剪枝是一种动态的内存管理策略，其核心在于通过设计有效的显著性度量（Saliency Metrics）来识别并驱逐上下文中“最不重要”的Token所对应的KV对，从而在有限的内存预算内最大化有效上下文的长度。</li><li><strong>一个丰富的优化生态</strong>：智能剪枝并非唯一的解决方案。它与KV量化、低秩近似、架构修改（如GQA）等技术共同构成了一个多维度的优化生态系统。这些技术因其作用维度不同而具有正交性，为构建复合优化策略提供了可能 16。</li></ol><h3 id="_6-2-智能剪枝的实际影响" tabindex="-1"><a class="header-anchor" href="#_6-2-智能剪枝的实际影响"><span>6.2 智能剪枝的实际影响</span></a></h3><p>智能剪枝及其相关技术对LLM的实际部署产生了深远的影响。研究表明，有效的剪枝策略能够带来显著的性能提升：</p><ul><li><strong>提升吞吐量</strong>：通过节省内存，可以在相同的硬件上运行更大的批量大小。例如，PagedEviction在LLaMA-1B模型上实现了高达37%的吞吐量提升（从2200 tokens/sec提高到3020 tokens/sec）13。</li><li><strong>降低延迟</strong>：更高效的内存管理可以减少计算开销，从而降低单次生成的延迟，报告显示在不同尺寸模型上可实现约10-12%的延迟缩减 13。</li><li><strong>解锁长上下文能力</strong>：这是最重要的影响之一。剪枝技术使得在内存受限的设备上处理数万甚至更长的Token序列成为可能，这对于文档分析、代码生成、检索增强生成（RAG）等需要长依赖关系的应用至关重要 3。</li></ul><h3 id="_6-3-未来方向与开放性研究问题" tabindex="-1"><a class="header-anchor" href="#_6-3-未来方向与开放性研究问题"><span>6.3 未来方向与开放性研究问题</span></a></h3><p>尽管当前技术已取得显著进展，但KV缓存优化领域仍有广阔的探索空间。</p><ul><li><strong>复合策略研究</strong>：正如第五部分所强调的，未来的趋势在于组合不同的优化技术。如何自动地、动态地为特定模型、硬件和任务设计最优的“优化堆栈”（例如，一个使用GQA训练的模型，在推理时对远距离上下文采用低秩近似，对近期上下文采用量化剪枝），是一个极具价值的研究方向。</li><li><strong>自适应预算</strong>：当前的剪枝策略大多使用固定的内存预算。未来的研究可以探索自适应预算机制，使缓存大小能根据当前任务的复杂度或生成内容的信息密度动态调整。</li><li><strong>硬件协同设计</strong>：随着AI专用硬件的发展，未来可以设计原生支持稀疏注意力计算或动态缓存管理的硬件加速器，从底层彻底改变性能-内存的权衡曲线。</li><li><strong>理论基础的深化</strong>：目前，大多数显著性度量仍然是基于经验或代理的。我们需要更深入的理论来理解在特定任务中，上下文信息的哪些部分是真正不可或缺的。发展一种更具普适性的上下文显著性理论，将指导我们设计出更精准、更鲁棒的剪枝算法。</li></ul><p>总之，智能剪枝是应对大型语言模型内存挑战的关键技术。它不仅是一系列算法的集合，更体现了在复杂AI系统中，算法、软件和硬件之间协同优化的设计哲学。随着模型规模和应用场景的不断扩展，对KV缓存进行更高效、更智能的管理，将持续作为推动人工智能技术边界向前发展的核心驱动力之一。</p>`,106)])])}const r=a(p,[["render",i]]),o=JSON.parse('{"path":"/notes/Misc/zuxsupvd/","title":"智能剪枝深度解析：大型语言模型KV缓存优化技术白皮书","lang":"zh-CN","frontmatter":{"title":"智能剪枝深度解析：大型语言模型KV缓存优化技术白皮书","createTime":"2026/01/08 12:38:42","permalink":"/notes/Misc/zuxsupvd/"},"readingTime":{"minutes":29.8,"words":8939},"git":{"createdTime":1767847587000,"updatedTime":1767847587000,"contributors":[{"name":"salt235","username":"salt235","email":"734489881@qq.com","commits":1,"avatar":"https://avatars.githubusercontent.com/salt235?v=4","url":"https://github.com/salt235"}]},"filePathRelative":"notes/Misc/记录/智能剪枝深度解析：大型语言模型KV缓存优化技术白皮书.md","headers":[]}');export{r as comp,o as data};
