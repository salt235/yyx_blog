import{_ as a,c as n,a as t,o as e}from"./app-DCYTdW2o.js";const p={};function i(r,s){return e(),n("div",null,[...s[0]||(s[0]=[t('<blockquote><p>最早的transformer = encoder + decoder</p></blockquote><h2 id="cnn与nlp的形象类比关系" tabindex="-1"><a class="header-anchor" href="#cnn与nlp的形象类比关系"><span>CNN与NLP的形象类比关系</span></a></h2><p><strong>CNN:</strong></p><p>针对一个图片，可以理解为一张二维的像素图，有多个通道。</p><p><strong>NLP:</strong></p><p>针对一句话，拆分为n个token，每个 token 经过 embedding 后，表示为一个固定维度的向量。</p><p>每个token可以理解为CNN中的一个像素点（贯穿多个通道，通道数即 embedding 维度）。</p><p>所有的token合起来就是一个像素图（贯穿多个通道）。</p><h2 id="layer-normalization" tabindex="-1"><a class="header-anchor" href="#layer-normalization"><span>Layer Normalization</span></a></h2><p>Batch Normalization (BN) 虽然在 CNN 上效果很好，但在以下场景存在致命缺陷，促使了 LN 的诞生：</p><ol><li><strong>RNN/LSTM 等序列模型</strong>：序列长度不固定。如果在不同时间步做 BN，统计量会波动极大且难以维护。</li><li><strong>小 Batch Size (甚至 Batch=1)</strong>：大模型（如 LLM）训练时显存受限，Batch Size 往往很小，此时 BN 的统计量估算失效。</li></ol><p>LN 通过<strong>抛弃对 Batch 维度的依赖</strong>，完美解决了上述问题。</p><h3 id="_1-ln-和-bn-的区别" tabindex="-1"><a class="header-anchor" href="#_1-ln-和-bn-的区别"><span>1. LN 和 BN 的区别</span></a></h3><p>很像，但是：</p><p>假设输入张量形状为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mi>C</mi><mo separator="true">,</mo><mi>H</mi><mo separator="true">,</mo><mi>W</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, C, H, W)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mclose">)</span></span></span></span></p><ul><li><p><strong>BN (纵向切)</strong>：固定 Channel，跨越 Batch (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span>) 统计。</p><p>在cnn中，可以想象成把一个通道的那张图norm。BN 非常适合 CNN。</p></li><li><p><strong>LN (横向切)</strong>：固定 Sample (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span>)，跨越 Feature (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mo separator="true">,</mo><mi>H</mi><mo separator="true">,</mo><mi>W</mi></mrow><annotation encoding="application/x-tex">C, H, W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span></span></span></span>) 统计。</p><p>在cnn中，可以想象成定一个点，然后垂直和每一个通道的该位置点，合起来一起norm。但是 LN <strong>在 CNN 上表现一般</strong>：在图像分类等任务中，LN 往往打不过 BN。</p><ul><li><em>原因</em>：图像的特征（Channel）之间差异很大（比如边缘检测 vs 颜色检测），强制把它们拉到一个分布可能会破坏图像的空间/语义信息。</li></ul></li></ul><h3 id="_2-对于transformer-用ln" tabindex="-1"><a class="header-anchor" href="#_2-对于transformer-用ln"><span>2. 对于Transformer，用LN</span></a></h3><p>原因：</p><ol><li><strong>序列长度不定</strong>：NLP 句子有长有短，BN 需要对齐长度（Padding），Padding 的 0 值会严重干扰 BN 的均值方差计算。</li><li><strong>样本统计不稳</strong>：NLP 数据的 Batch 统计量往往不如图像稳定，且 Batch Size 受显存限制通常较小，BN 效果差。</li></ol><h2 id="encoder" tabindex="-1"><a class="header-anchor" href="#encoder"><span>encoder</span></a></h2><h3 id="bert结构" tabindex="-1"><a class="header-anchor" href="#bert结构"><span>Bert结构</span></a></h3><p><img src="https://cdn.jsdelivr.net/gh/salt235/tuchuang@main/img/QQ_1767016854328.png" alt="早期的一个encoder结构"></p><p>上图其实就是Bert的结构，Bert本质上就是一个编码器。</p><p>输入是四个向量，输出是四个处理后的特征向量（包含了上下文信息，且与x一一对应）。</p><ul><li><p>首先对输入的向量，进行<strong>位置编码</strong>，然后送入一个多头的Attention，让模型在处理当前单词时，能够“关注”到句子里的其他单词。</p></li><li><p>然后<strong>Add</strong>: 指的是 Residual Connection (残差连接)。看旁边的箭头，输入直接绕过了 Attention 层加到了输出上。这能防止网络过深导致的退化。同时<strong>Norm</strong>: 指的就是 <strong>Layer Normalization (LN)</strong>。</p></li><li><p>接着进入一个就是一个简单的<strong>全连接网络（MLP）</strong>，对特征进行进一步的非线性变换。</p></li><li><p>最后再次Add和Norm，输出。</p></li></ul><p>BERT 就是一堆 <strong>Transformer Encoder</strong> 叠起来。每个 Encoder 层里有两步核心操作：先做<strong>注意力 (Attention)</strong>，再做<strong>前馈 (Feed Forward)</strong>，每一步做完都要记得 <strong>Add &amp; Norm (残差+层归一化)</strong>。</p><blockquote><p>在 BERT/Transformer 的图里，“前馈”就是一个<strong>负责特征提取和非线性变换的“加工厂”</strong>。它没有记忆功能（不存状态），只是单纯地把上一层给它的数据处理好，然后扔给下一层。</p></blockquote><h2 id="decoder" tabindex="-1"><a class="header-anchor" href="#decoder"><span>Decoder</span></a></h2><h3 id="decoder的结构" tabindex="-1"><a class="header-anchor" href="#decoder的结构"><span>Decoder的结构</span></a></h3><p><img src="https://cdn.jsdelivr.net/gh/salt235/tuchuang@main/img/QQ_1767949811153.png" alt="decoder"></p><p>Transformer 的 Decoder 是一种用于序列生成的网络结构，<br> 它通过“因果自注意力（Causal Self-Attention）”实现 autoregressive（自回归）建模。</p><h3 id="masked-self-attention" tabindex="-1"><a class="header-anchor" href="#masked-self-attention"><span>Masked Self-attention</span></a></h3><p><img src="https://cdn.jsdelivr.net/gh/salt235/tuchuang@main/img/QQ_1767949111534.png" alt="Masked Self-attention"></p><p>和Self-attention的区别是，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>b</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">b_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>的生成之和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>a</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">a_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>到<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>a</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">a_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>有关，不考虑后面的。</p><h4 id="为什么要masked" tabindex="-1"><a class="header-anchor" href="#为什么要masked"><span>为什么要masked？</span></a></h4><p>因为Decoder的输入不是一下子把所有的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>a</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">a_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>都给出来，而且先<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>a</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">a_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，再一个一个到<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>a</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">a_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。</p><h2 id="encoder和decoder的连接" tabindex="-1"><a class="header-anchor" href="#encoder和decoder的连接"><span>Encoder和Decoder的连接</span></a></h2><p><img src="https://cdn.jsdelivr.net/gh/salt235/tuchuang@main/img/QQ_1767950220351.png" alt="encoder和decoder的连接"></p><h3 id="cross-attention" tabindex="-1"><a class="header-anchor" href="#cross-attention"><span>Cross attention</span></a></h3><p><img src="https://cdn.jsdelivr.net/gh/salt235/tuchuang@main/img/QQ_1767950312059.png" alt="cross attention"></p><p>Cross-Attention 是Decoder 在生成当前 token 时，去“查询（Query）Encoder 输出”的机制。</p><p>Decoder产生Q去查询Encoder的K和。</p><h2 id="seq2seq模型的训练" tabindex="-1"><a class="header-anchor" href="#seq2seq模型的训练"><span>seq2seq模型的训练</span></a></h2><blockquote><p>Seq2Seq（Encoder–Decoder）模型的训练方式是： 用“已知的目标序列”作为 Decoder 的输入， 通过 autoregressive 预测下一个 token， 对每一步计算交叉熵损失，并整体反向传播。</p></blockquote><p>这套方法有个名字：<strong>Teacher Forcing</strong>。</p><h3 id="为什么什么用-已知的目标序列-作为训练decoder的输入呢" tabindex="-1"><a class="header-anchor" href="#为什么什么用-已知的目标序列-作为训练decoder的输入呢"><span>为什么什么用“已知的目标序列”作为训练Decoder的输入呢？</span></a></h3><p>Decoder 训练时输入目标序列，不是让模型“抄答案”，而是让它在“正确历史条件下学习如何预测下一个 token”，这使得序列生成可以并行训练。</p><h3 id="暴露偏差-exposure-bias" tabindex="-1"><a class="header-anchor" href="#暴露偏差-exposure-bias"><span>暴露偏差（Exposure Bias）</span></a></h3><p>模型训练时只“暴露”在真实历史上， 推理时却必须在“自己生成的历史”上继续预测。<strong>明知道有暴露偏差，为什么还要 Teacher Forcing？</strong></p><h4 id="_1-不-teacher-forcing-根本训不动" tabindex="-1"><a class="header-anchor" href="#_1-不-teacher-forcing-根本训不动"><span>1. 不 teacher forcing，根本训不动</span></a></h4><ul><li>初期模型输出几乎是随机</li><li>输入错误上下文 → 梯度噪声巨大</li><li>训练不稳定，甚至发散</li></ul><h4 id="_2-transformer-需要并行训练" tabindex="-1"><a class="header-anchor" href="#_2-transformer-需要并行训练"><span>2. Transformer 需要并行训练</span></a></h4><ul><li>自回归采样：必须一步步生成</li><li>Teacher forcing：<strong>一次 forward 算所有位置</strong></li></ul><h4 id="_3-实际效果好" tabindex="-1"><a class="header-anchor" href="#_3-实际效果好"><span>3. 实际效果好</span></a></h4><ul><li>大模型 + 大数据</li><li>暴露偏差在实践中被“规模”压住了</li></ul>',55)])])}const m=a(p,[["render",i]]),o=JSON.parse('{"path":"/notes/Lee_DL/w48llyos/","title":"transformer","lang":"zh-CN","frontmatter":{"title":"transformer","createTime":"2025/12/29 20:20:56","permalink":"/notes/Lee_DL/w48llyos/"},"readingTime":{"minutes":4.4,"words":1321},"git":{"createdTime":1767017777000,"updatedTime":1769490793000,"contributors":[{"name":"salt235","username":"salt235","email":"734489881@qq.com","commits":4,"avatar":"https://avatars.githubusercontent.com/salt235?v=4","url":"https://github.com/salt235"}]},"filePathRelative":"notes/Lee_DL/Attention-based Models/transformer.md","headers":[]}');export{m as comp,o as data};
