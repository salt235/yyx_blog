import{_ as e,c as n,a,o as s}from"./app-DCYTdW2o.js";const o={};function i(r,t){return s(),n("div",null,[...t[0]||(t[0]=[a('<blockquote><p>监督学习和自监督学习：</p><ul><li><p><strong>监督学习（Supervised Learning）</strong>： 使用<strong>人工标注的标签</strong>进行训练。</p></li><li><p><strong>自监督学习（Self-Supervised Learning）</strong>： <strong>不依赖人工标注</strong>，通过数据本身构造监督信号。</p></li></ul></blockquote><h2 id="_1-masking-input" tabindex="-1"><a class="header-anchor" href="#_1-masking-input"><span>1. Masking Input</span></a></h2><blockquote><p>通过随机遮盖部分输入 token，让模型在双向上下文条件下预测被遮盖的词。</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/salt235/tuchuang@main/img/QQ_1769492550609.png" alt="Masking Inpput"></p><h3 id="mask-策略-bert-原始设定" tabindex="-1"><a class="header-anchor" href="#mask-策略-bert-原始设定"><span>Mask 策略（BERT 原始设定）</span></a></h3><p>对被选中的 token（约 15%）：</p><ul><li>80% → 替换为 <code>[MASK]</code></li><li>10% → 替换为随机 token</li><li>10% → 保持原 token 不变</li></ul><blockquote><p>目的： <strong>缓解训练阶段与推理阶段分布不一致问题</strong></p></blockquote><h3 id="训练目标" tabindex="-1"><a class="header-anchor" href="#训练目标"><span>训练目标</span></a></h3><ul><li>对被 Mask 的位置计算交叉熵损失</li><li>非 Mask 位置不计入 loss</li></ul><h2 id="_2-next-sentence-prediction" tabindex="-1"><a class="header-anchor" href="#_2-next-sentence-prediction"><span>2. Next Sentence Prediction</span></a></h2><p><img src="https://cdn.jsdelivr.net/gh/salt235/tuchuang@main/img/QQ_1769493037241.png" alt="Next Sentence Prediction"></p><h3 id="训练目标-1" tabindex="-1"><a class="header-anchor" href="#训练目标-1"><span>训练目标</span></a></h3><ul><li>使用 <code>[CLS]</code> token 的输出表示</li><li>进行二分类预测：</li></ul><table><thead><tr><th>标签</th><th>含义</th></tr></thead><tbody><tr><td>IsNext</td><td>B 是 A 的下一句</td></tr><tr><td>NotNext</td><td>B 不是 A 的下一句</td></tr></tbody></table><blockquote><p>从语料中构造句子对 <code>(Sentence A, Sentence B)</code>：</p><ul><li>50%：B 是 A 的下一句（IsNext）</li><li>50%：B 是随机抽取的句子（NotNext）</li></ul></blockquote><h2 id="_3-fine-tune" tabindex="-1"><a class="header-anchor" href="#_3-fine-tune"><span>3. Fine-tune</span></a></h2><p>将bert用于下游任务，此时不再是自监督学习，而是需要人工标注的监督学习。例如下面这几个问题：</p><h3 id="序列分类" tabindex="-1"><a class="header-anchor" href="#序列分类"><span>序列分类</span></a></h3><p><img src="https://cdn.jsdelivr.net/gh/salt235/tuchuang@main/img/QQ_1769493667408.png" alt="Case 1"></p><ul><li><p><strong>BERT 当编码器用</strong> 输入一句话（在最前面加 <code>[CLS]</code>），BERT 输出每个 token 的表示。</p></li><li><p><strong>用 <code>[CLS]</code> 表示整句话</strong> 取最后一层的 <code>[CLS]</code> 向量，作为整句的语义表示。</p></li><li><p><strong>接一个简单分类头并微调</strong> 在 <code>[CLS]</code> 上接一个 <strong>线性层（随机初始化）</strong> 输出类别； 训练时 <strong>分类头 + BERT 参数一起更新</strong>。 预训练权重是“比随机初始化好得多的预训练起点”</p></li></ul><h3 id="词性标注" tabindex="-1"><a class="header-anchor" href="#词性标注"><span>词性标注</span></a></h3><p><img src="https://cdn.jsdelivr.net/gh/salt235/tuchuang@main/img/QQ_1769493959900.png" alt="Case 2"></p><ul><li><p><strong>每个 token 都要预测一个标签</strong> 输入是一个序列，输出长度<strong>和输入一样</strong>。</p></li><li><p><strong>不用 <code>[CLS]</code> 做预测</strong><code>[CLS]</code> 只作为辅助上下文； <strong>真正用于预测的是每个 token 自己的隐藏向量</strong>。</p></li><li><p><strong>每个 token 接同一个线性分类头</strong></p><p>对每个 token 的输出向量接一个 Linear，参数共享输出该 token 的类别。</p></li><li><p><strong>端到端微调</strong> Linear 层随机初始化，<strong>BERT + Linear 一起训练</strong>。</p></li></ul>',24)])])}const c=e(o,[["render",i]]),d=JSON.parse('{"path":"/notes/Lee_DL/61oxdhhi/","title":"Bert","lang":"zh-CN","frontmatter":{"title":"Bert","createTime":"2026/01/27 06:48:21","permalink":"/notes/Lee_DL/61oxdhhi/"},"readingTime":{"minutes":1.84,"words":553},"git":{"createdTime":1769496485000,"updatedTime":1769496485000,"contributors":[{"name":"salt235","username":"salt235","email":"734489881@qq.com","commits":1,"avatar":"https://avatars.githubusercontent.com/salt235?v=4","url":"https://github.com/salt235"}]},"filePathRelative":"notes/Lee_DL/Attention-based Models/Bert.md","headers":[]}');export{c as comp,d as data};
